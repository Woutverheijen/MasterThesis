{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bf092",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "FRIDAY_PATH = \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
    "WEDNESDAY_PATH = \"Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "\n",
    "df_fri = pd.read_csv(FRIDAY_PATH)\n",
    "df_wed = pd.read_csv(WEDNESDAY_PATH)\n",
    "\n",
    "data = pd.concat([df_fri, df_wed], ignore_index=True)\n",
    "print(f\"Combined shape: {data.shape}\")\n",
    "\n",
    "#Data Prepocessing\n",
    "data.columns = data.columns.str.strip()\n",
    "if 'Label' not in data.columns:\n",
    "    raise ValueError(\"Expected 'Label' column not found.\")\n",
    "\n",
    "print(\"Unique labels before binary encoding:\")\n",
    "print(data['Label'].unique())\n",
    "\n",
    "data['Label'] = data['Label'].astype(str).str.strip()\n",
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == \"BENIGN\" else 1)\n",
    "\n",
    "print(\"\\nBinary label distribution (0=BENIGN, 1=ATTACK):\")\n",
    "print(data['Label'].value_counts())\n",
    "\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "before = data.shape[0]\n",
    "data.dropna(inplace=True)\n",
    "after = data.shape[0]\n",
    "\n",
    "print(f\"\\nDropped {before - after} rows due to NaN/inf values.\")\n",
    "print(f\"Remaining rows: {after}\")\n",
    "\n",
    "zero_var_cols = [c for c in data.columns if c != 'Label' and data[c].nunique() <= 1]\n",
    "if zero_var_cols:\n",
    "    print(\"Dropping zero-variance columns:\", zero_var_cols)\n",
    "    data.drop(columns=zero_var_cols, inplace=True)\n",
    "\n",
    "feature_names = [c for c in data.columns if c != 'Label']\n",
    "X = data[feature_names].values\n",
    "y = data['Label'].values\n",
    "\n",
    "print(f\"\\nFinal feature count: {len(feature_names)}\")\n",
    "print(f\"Final dataset size  : {data.shape[0]} rows\")\n",
    "\n",
    "#Model Training\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Shapes -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "#Small hyperparamater search\n",
    "xgb_base_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"random_state\": SEED,\n",
    "    \"n_estimators\": 300,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "xgb_param_grid = [\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"deeper_more_trees\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 400,\n",
    "            \"max_depth\": 8,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"shallower_faster\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 4,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lower_lr_more_trees\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 500,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"more_randomness\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "xgb_best_val_acc = -1.0\n",
    "xgb_best_name = None\n",
    "xgb_best_params = None\n",
    "xgb_best_model = None\n",
    "\n",
    "for i, cfg in enumerate(xgb_param_grid, 1):\n",
    "    print(f\"\\n=== XGBoost Config {i}/{len(xgb_param_grid)}: {cfg['name']} ===\")\n",
    "\n",
    "    params = xgb_base_params.copy()\n",
    "    params.update(cfg[\"params\"])\n",
    "\n",
    "    xgb_tmp = XGBClassifier(**params)\n",
    "\n",
    "    xgb_tmp.fit(X_train, y_train)\n",
    "\n",
    "    y_val_pred = xgb_tmp.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Validation accuracy for {cfg['name']}: {val_acc:.6f}\")\n",
    "\n",
    "    if val_acc > xgb_best_val_acc:\n",
    "        xgb_best_val_acc = val_acc\n",
    "        xgb_best_name = cfg[\"name\"]\n",
    "        xgb_best_params = params\n",
    "        xgb_best_model = xgb_tmp\n",
    "\n",
    "print(\"\\n=== Best XGBoost configuration selected ===\")\n",
    "print(\"Name :\", xgb_best_name)\n",
    "print(\"Params:\", xgb_best_params)\n",
    "print(f\"Best validation accuracy: {xgb_best_val_acc:.6f}\")\n",
    "\n",
    "xgb_clf = xgb_best_model\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "prec_xgb = precision_score(y_test, y_pred_xgb, zero_division=0)\n",
    "rec_xgb = recall_score(y_test, y_pred_xgb, zero_division=0)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, zero_division=0)\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "tn, fp, fn, tp = cm_xgb.ravel()\n",
    "far_xgb = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "print(\"\\n=== XGBoost – Test Metrics (Binary: 0=BENIGN, 1=ATTACK) ===\")\n",
    "print(f\"Accuracy : {acc_xgb:.4f}\")\n",
    "print(f\"Precision: {prec_xgb:.4f}\")\n",
    "print(f\"Recall   : {rec_xgb:.4f}\")\n",
    "print(f\"F1-score : {f1_xgb:.4f}\")\n",
    "print(f\"FAR      : {far_xgb:.6f}  (False Alarm Rate = FP / (FP + TN))\")\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"XGBoost – Confusion Matrix (Binary)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Global Feature Importance\n",
    "xgb_importances = xgb_clf.feature_importances_\n",
    "xgb_feat_imp_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": xgb_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 XGBoost features:\")\n",
    "print(xgb_feat_imp_df.head(10))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=xgb_feat_imp_df.head(10), x=\"Importance\", y=\"Feature\")\n",
    "plt.title(\"XGBoost – Top 10 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
